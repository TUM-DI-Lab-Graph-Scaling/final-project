\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

Large-scale model training on massive training data is the way to achieve top-tier 
accuracy, but are difficult to train due to memory constraints and communication overhead 
when scaling out to multiple devices. Much of ongoing research is currently limited to 
training giant transformer models such as Bert or GPT-3. However, other types of machine 
learning models also benefit from scaling up the architectures and model parameters, 
such as the application of Graph Neural Networks (GNN) to atomic simulations. In our work, 
we use optimizations from Microsoft Deepspeed, a toolkit for efficiently training 
large-scale models, to scale up and more efficiently train GNNs for predicting energies 
and forces of catalysts. Our integration of Deepspeed into GNN architectures includes 
mixed precision training, offloading of optimizer states and parameters to CPU or Non-volatile 
memory, and most notably the Zero Redundancy Optimizer (ZeRO) to reduce the amount of 
replicated states during data parallel training. We evaluated our optimizations by increasing 
the size of the two most successful GNN architectures for atomic simulations, DimeNet 
and GemNet, to several hundred million parameters and train them on the OC20 dataset of 
the Open Catalyst Project. The performed optimization of our approach show up to 
50\% less reserved memory and 87\% less allocated memory on GemNet. However, we also observe 
weaknesses in ZeRO such as increased communication costs and slower 
runtimes when using offloading strategies. 