\subsection{Zero Redundancy Optimizer (ZeRO)}

The Zero Redundancy Optimizer (ZeRO) is a scaling engine on top of data-parallel training for 
machine learning models. In short, ZeRO removes memory redundancies by partitioning states which 
are used accross all processes such as the optimizer states, gradients and model parameters. 
Since all processes only have to maintain a shard of states, ZeRO should use less memory while 
retaining computational granularity and communication overhead compared to traditional Data 
Parallelism.

Deepspeed allows developers to integrate ZeRO into arbitrary models which are trained using an 
DP pipeline. Using the Deepspeed configuration file, the 

\subsubsection{Stage 1 (OS)}

Stage 1 of ZeRO enables Optimizer State Partitioning. 

\subsubsection{Stage 2 (OS+G)}

Stage 2 of ZeRO partitions both optimizer states and gradients. ZeRO-2 generally follows the following 
execution path for each step: Each GPU holds a replica of the entire model (all parameters), however 
only a mutually exlusive portion of the parameters are updated on each of the devices. The GPUs only store 
the shard of gradients and optimizer states, which they need for their particular portion of parameters 
during the forward and backward pass. After doing the optimizer step, the GPUs update each other through 
an \textit{all-gather} nccl communication collective. 

\subsubsection{Stage 3 (OS+G+P)}

Stage 3 of ZeRO partitions optimizer states, gradients and model parameters, thus merging the 
traditional Data Parallelism approach with Model parallelism.

\begin{python}
for param in self.model.parameters():
    deepspeed.zero.register_external_parameter(
        self.model, param
    )
\end{python}


\input{figures/deepspeed/zero.tex}

After our integration, the ZeRO optimization can be enabled for all OCP models in the Deepspeed configuration 
file. The specification is as follows:

\begin{json}
{
    "bf16": {
        "enabled": "true"
    }
    "zero_optimization": {
        "stage": [1|2|3]
    }
}
\end{json}

Note that the current Deepspeed implementation requires bfloat16 floating point precision to be enabled. 
