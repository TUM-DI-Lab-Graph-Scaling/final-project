\section{Implementation}
\label{section:implementation}

\subsection{Usage of DeepSpeed}

Our goal for this project was to use the DeepSpeed engine to speed up and scale 
GNNs. The Open Catalyst Project presents the ideal 
benchmark for our acceleration approaches since it contains several model definitions 
for GNNs in the context of molecular simulations. Previous research has shown 
that scaling OCP models to multiple hundreds of millions of parameters 
leads to better accuracies but requires ever larger infrastructure
and ever more time for training \cite{https://doi.org/10.48550/arxiv.2203.09697}. 
Therefore we want to utilize 
DeepSpeed to scale beyond standard data parallelism and enable training of billion 
parameter models.

We took multiple steps to integrate DeepSpeed into the OCP project, which we will 
describe in the following section. All of our changes are available on GitHub in our 
organization fork of OCP\footnote{\url{https://github.com/TUM-DI-Lab-Graph-Scaling/ocp}}.

To start off, we initialized the DeepSpeed engine. 
The initialization in our code looks roughly like this:

\begin{python}
self.model, self.optimizer, _, _ = deepspeed.initialize(
    config=self.config["deepspeed_config"],
    model=self.model,
    model_parameters=self.model.parameters(),
    optimizer=self.optimizer,
)
\end{python}

DeepSpeed provides a model and optimizer wrapper to offer the same PyTorch API as 
before, but performs optimizations under the hood when forward, backward and step 
functions are invoked. Furthermore, we had to replace the original distributed 
environment setup with DeepSpeed's distributed setup. An example of why DeepSpeed 
needs a custom distributed environment is to spawn CPU worker processes for optimizer 
or parameter offloading. We 
will dive deeper into this optimization in Section~\ref{subsection:offloading}.

To enable or disable specific features of the optimizer engine, DeepSpeed is 
configured using an extra JSON file. We extended the previous OCP configuration 
code to handle the DeepSpeed config as additional argument and pass it through 
to the DeepSpeed engine at the beginning of training. This also allows us to 
make feature specific changes to the codebase, since we also parse the config and 
make it available at runtime.

An example DeepSpeed configuration file looks like this:

\begin{json}
{
    "train_batch_size": 16,
    "train_micro_batch_size_per_gpu": 2,
    "gradient_accumulation_steps": 1,
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 3,
        "contiguous_gradients": true
    }
}
\end{json}

In this configuration, \texttt{train\_micro\_batch\_size\_per\_gpu} refers to the 
batch size processed by one GPU in one step. \texttt{gradient\_accumulation\_steps} 
refers to the number of steps to accumulate gradients before averaging and applying 
them. \texttt{train\_batch\_size} is the effective batch size per accumulation step, 
so 
\begin{align*}
    \text{\texttt{train\_batch\_size}} \; &= \; 
    \# \: \text{GPUs} \cdot \text{\texttt{train\_micro\_batch\_size\_per\_gpu}} \\
    &\qquad \cdot \text{\texttt{gradient\_accumulation\_steps}} \text{.}
\end{align*}

We introduce the other configurations in later sections. \texttt{fp16} and 
\texttt{bf16} are mixed-precision configurations and are explained in Section~
\ref{subsection:mixedprecision}. All the configurations in \texttt{zero\_optimization} 
are introduced in Section~\ref{subsection:zero} and amended in Section~
\ref{subsection:offloading} by offload features.