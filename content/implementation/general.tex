\section{Implementation}
\label{section:implementation}

\subsection{Usage of DeepSpeed}

Our goal for this project was to use the Deepspeed engine to speed up and scale 
large-scale Graph Neural Networks. The Open Catalyst Project presents the ideal 
benchmark for our acceleration approaches since it contains several model definitions 
for GNNs in the context of molecular simulations. Previous research has shown 
that scaling OCP models to multiple hundreds of millions of parameters or even 
billions of parameters leads to better accuracies. Therefore we want to utilize 
Deepspeed to scale beyond standard Data Parallelism and enable training of billion 
parameter models.

We took multiple steps to integrate Deepspeed into the OCP project, which we will 
descript in the following section. All of our changes are available on Github in our 
organization fork of OCP\footnote{\url{https://github.com/TUM-DI-Lab-Graph-Scaling/ocp}}.

To start off, we initialized the Deepspeed engine. 
The initialization in our code looks roughly like this:

\begin{python}
self.model, self.optimizer, _, _ = deepspeed.initialize(
                    config=self.config["deepspeed_config"],
                    model=self.model,
                    model_parameters=self.model.parameters(),
                    optimizer=self.optimizer,
                )
\end{python}

Deepspeed provides a model and optimizer wrapper, offering the same PyTorch API as 
before, but performs optimizations under the hood when forward, backward and steps 
functions are invoked. Furthermore, we had to replace the original distributed 
environment setup with Deepspeeds distributed setup, which particulary useful for 
Deepspeed to spawn CPU worker processes for optimizer or parameter offloading. We 
will dive deeper into this optimization in Section \ref*{subsection:offloading}.


\begin{json}
{
    "train_batch_size": 16,
    "train_micro_batch_size_per_gpu": 2,
    "gradient_accumulation_steps": 1,
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 3,
        "contiguous_gradients": true
    }
}
\end{json}