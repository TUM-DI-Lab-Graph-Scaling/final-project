\subsection{Mixed Precision Training}
\label{subsection:mixedprecision}

Mixed precision training uses lower bit floating point types for optimizer states, gradients and model parameters to 
reduce the memory footprint during model training. 

PyTorch offers out-of-the-box support for two types of mixed precision training: Floating point with 16 bits (FP16) and 
brain floating point with 16 bits (BFloat16) which we will introduce in the next subsections and explain our integration 
into the OCP project.

\subsubsection{FP16}

FP16 is a floating-point number format which uses 5 bits for the exponent, 10 bits for the fraction and 1 bit for the 
sign. This halves the used memory compared to the default single-precision floating-point format which uses 8 bits 
for the exponent and 24 bits for the fraction. The specification of FP16 can be found in the official IEEE 754-2008 
standard\footnote{\url{https://standards.ieee.org/ieee/754/4211/}}.
FP16 lowers the precision and reduces the range of numbers that can be represented, making calculations less 
accurate and increasing the risk of overflows/underflows.

PyTorch and DeepSpeed offer automatic mixed precision for gradients. Developers can define regions with context managers 
to allow PyTorch to automatically choose the precision for GPU operations. Even though autocast worked for learnable
parameters of the model as well as gradients, we had to convert input data and all constants defined in the models to half
precision manually. With our adaptions, 
FP16 can be used by just setting the \texttt{fp16} flag in the DeepSpeed configuration.

\subsubsection{BFloat16}

As mentioned before, the use of FP16 leads to a higher risk of overflows and underflows. We experienced many overflows 
of gradients when using FP16 with DeepSpeed. To avoid overflows, DeepSpeed offers support for the Bfloat16
\footnote{\url{https://cloud.google.com/tpu/docs/bfloat16}}
number format which allocates more bits to the exponent to extend the range of representable numbers while lowering 
precision.
As with FP16, we had to include manual type casts in the OCP codebase to integrate Bfloat16 support. 
Bfloat16 can also be enabled from the DeepSpeed configuration using the \texttt{bf16} flag.

\input{figures/mixed_precision/half_precision.tex}

