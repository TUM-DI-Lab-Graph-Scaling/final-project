\subsection{Mixed Precision Training}
\label{subsection:mixedprecision}

Mixed precision training uses lower bit floating point types for optimizer states, gradients and model parameters to 
reduce the memory footprint during model training. 

PyTorch offers out-of-the-box support for two types of mixed precision training: Floating point with 16 bits (FP16) and 
brain floating point with 16 bits (BFloat16) which we will introduce in the next sections and explain our integration 
into the OCP project.

\subsubsection{FP16}

FP16 is a floating-point number format which uses 5 bits for the exponent, 10 bits for the fraction and 1 bit for the 
sign. This halves the used memory compared to the default single-precision floating-point format which uses 8 bits 
for the exponent and 24 bits for the fraction. However, FP16 lowers the precision and reduces the range of numbers 
that can be represented, making calculations less accurate and increasing the risk of overflows/underflows.

\input{figures/mixed_precision/fp16.tex}

PyTorch and Deepspeed offer automatic mixed precision for gradients. Developers can regions with context managers 
to allow PyTorch to automatically choose the precision for GPU operations. Unfortunately both autocast modules from 
both PyTorch and Deepspeed failed to detect parameters and gradients in the OCP models when we applied the context 
manager. Our solution was to manually cast parameters to half precision when Deepspeed is used. After our adaptions 
FP16 can be used by just setting one flag in the Deepspeed configuration:

\begin{json}
"fp16": {
    "enabled": true
}
\end{json}

\subsubsection{BFloat16}

As mentioned before, the use of FP16 leads to a higher risk of overflows/underflows. We experienced many overflows 
of gradients when using FP16 with Deepspeed. To avoid overflows Deepspeed offers support for the Bfloat16 number 
format which allocates more bits to the exponent to extent the range of representable numbers while lowering 
precision.

\input{figures/mixed_precision/bloat16.tex}

As with FP16, we had to include manual type cast in the OCP codebase to integrate Bfloat16 support. 
Bfloat16 can also be enabled from the Deepspeed configuration

\begin{json}[language=json,firstnumber=1]
"bf16": {
    "enabled": true
}
\end{json}

