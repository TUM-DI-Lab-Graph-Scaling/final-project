\subsection{GNNs for Atomic Simulations}
\label{subsec:atomic-simulations}

\subsubsection{General}

Say one has information about the chemical and spatial structure of a molecule 
and would like to predict properties of this very molecule, e.g. its energy (at the 
current point in time or after some evolution) or the forces acting on the 
individual atoms. Some of these properties might be directly
computable using quantum mechanical methods---like Density Functional Theory, inter alia
to compute bond energies in molecules. However, some of these methods come
with significant drawbacks as they might be extremely expensive from a computational 
standpoint, imposing the necessity to either optimize these conventional methods 
as far as possible or use completely different approaches.

One alternative and very promising approach to compute or predict such molecules is to use supervised
machine learning in order to learn a function that maps molecule structures to their
respective properties based on data of known molecules. For this task, Graph
Neural Networks (GNNs) are very well suited not only because a molecule can naturally
be expressed as a graph (e.g. using its structural formula with atom numbers and
interatomic distances as node and edge attributes), but also because in this case, the 
assumption of invariance w.r.t. permutations of the atoms or the assumption that 
the update rules do not differ between nodes or edges and that one is hence
learning somewhat universal functions, seem very reasonable.

From now on, we define a molecule consisting of $n$ atoms by its atomic numbers 
$\zz$ and its positions $\XX$, where
\[
    \XX \: \coloneqq \: \set{\xx_1, \dots, \xx_n}, \; \xx_i \in \R^3
    \qquad \text{and} \qquad 
    \zz \: \coloneqq \: \set{z_1, \dots, z_n}, \; z_i \in \N \text{.}
\]
Nonetheless, in many cases there is still some arbitrariness in the choice of the 
exact positions $\XX$: Energies of molecule structures might be invariant w.r.t. 
translations and rotations, or forces might be invariant w.r.t. 
translations and equivariant w.r.t. rotations. Rather than letting the model figure out
these fundamental invariants of its predicting function learn on its own (e.g. by 
data augmentation), they are often directly incorporated into the model to reduce 
complexity. One approach to achieve this is to compute pairwise distances $d_{ij}$ 
between the atoms and use them instead of the individual atomic locations $\XX$, 
so---in the GN language (see \ref{subsubsec:gns})---define an input graph with the atomic numbers $\zz$ as 
node attributes and the interatomic distances as edge attributes (say, if a bond exists
between two atoms or the distance is below some threshold).

\input{./content/preliminaries/atomic_simulations/dimenet.tex}
\input{./content/preliminaries/atomic_simulations/gemnet.tex}

